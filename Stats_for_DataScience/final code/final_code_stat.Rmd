```{r}
# ===============================
# Required libraries
# ===============================
library(ggplot2)
library(matlib)
library(rsample)
library(MASS)

```

```{r}
# ===============================
# Import dataset
# ===============================
data <- read.csv("C:/Users/Dell/Desktop/Assignment_statistical_methods_for_DataScience/dataset_MSC7.csv", header = TRUE)

# Fix column name (if needed)
colnames(data)[7] <- "bg_plus_1h"

cat("=== Initial Data Check ===\n")
cat("Total rows:", nrow(data), "\n")
cat("Missing values per column:\n")
print(colSums(is.na(data)))

# ===============================
# HANDLE MISSING VALUES (MEDIAN IMPUTATION)
# =========================================

# Impute hr_mean with its median
missing_hr <- sum(is.na(data$hr_mean))
cat("\nMissing hr_mean values:", missing_hr, "\n")

if (missing_hr > 0) {
  hr_median_value <- median(data$hr_mean, na.rm = TRUE)
  data$hr_mean[is.na(data$hr_mean)] <- hr_median_value
  cat("hr_mean imputed with median:", round(hr_median_value, 2), "\n")
}

# Impute any other numeric columns with median
for (col in names(data)) {
  if (is.numeric(data[[col]]) && sum(is.na(data[[col]])) > 0) {
    data[[col]][is.na(data[[col]])] <- median(data[[col]], na.rm = TRUE)
  }
}

cat("\nMissing values after median imputation:\n")
print(colSums(is.na(data)))

# Input matrix (X)
X <- as.matrix(data[, 1:6])
colnames(X) <- c("bg_mean", "insulin_sum", "carbs_sum",
                 "hr_mean", "steps_sum", "cals_sum")

# Output vector (Y)
Y <- as.matrix(data[, 7])
colnames(Y) <- "bg_plus_1h"

N <- nrow(X)
```

```{r}
# Task 1.1
# Number of samples
N <- nrow(X)
# ===============================
# Time index based on data rows
# ===============================
time_index <- 1:N
# ===============================
# Convert to time series (optional, index-based)
# ===============================
X.ts <- ts(X)
Y.ts <- ts(Y)
# ===============================
# X-axis ticks based on row index
# ===============================
tick_positions <- seq(1, N, length.out = 10)
labels_display <- round(tick_positions)
# ===============================
# Plotting (3 rows × 2 columns)
# ===============================
par(mfrow = c(3, 2), mar = c(4, 4, 2, 1))

# Plot 1: bg_mean
plot(time_index, X[, "bg_mean"], type = "l",
     main = "bg_mean",
     xlab = "",
     ylab = "bg_mean",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)

# Plot 2: insulin_sum
plot(time_index, X[, "insulin_sum"], type = "l",
     main = "insulin_sum",
     xlab = "",
     ylab = "insulin_sum",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)

# Plot 3: carbs_sum
plot(time_index, X[, "carbs_sum"], type = "l",
     main = "carbs_sum",
     xlab = "Sample Index",
     ylab = "carbs_sum",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)

# Plot 4: hr_mean
plot(time_index, X[, "hr_mean"], type = "l",
     main = "hr_mean",
     xlab = "",
     ylab = "hr_mean",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)

# Plot 5: steps_sum
plot(time_index, X[, "steps_sum"], type = "l",
     main = "steps_sum",
     xlab = "",
     ylab = "steps_sum",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)

# Plot 6: cals_sum
plot(time_index, X[, "cals_sum"], type = "l",
     main = "cals_sum",
     xlab = "Sample Index",
     ylab = "cals_sum",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)

# Reset layout
par(mfrow = c(1, 1))

# ===============================
# Plot Output Signal
# ===============================
plot(time_index, Y, type = "l",
     main = "Time Series Plot of Output Signal",
     xlab = "Sample Index",
     ylab = "Blood Glucose (bg+1:00)",
     xaxt = "n")
axis(1, at = tick_positions, labels = labels_display)
```

```{r}
#task 1.2
# ===============================
# Distribution plots
# ===============================
# Histograms + densities for all inputs
par(mfrow = c(2, 3))
for (i in 1:6) {
  x_i <- X[, i]
  x_i <- x_i[!is.na(x_i)]
  hist(x_i, freq = FALSE,
       main = paste("Histogram & density (",colnames(X)[i],")" ),
       xlab = colnames(X)[i])
  lines(density(x_i), col = "red", lwd = 2)
}
par(mfrow = c(1, 1))

# Histogram + density for output
y_clean <- Y[!is.na(Y)]
hist(y_clean, freq = FALSE,
     main = "Histogram & density of Output (bg+1:00)",
     xlab = "Blood Glucose")
lines(density(y_clean), col = "blue", lwd = 2)

# Correlation matrix of inputs
cor_X <- cor(X, use = "complete.obs")
print(cor_X)

# Simple correlation heatmap
par(mfrow = c(1, 1))
image(
  1:ncol(cor_X), 1:ncol(cor_X), cor_X,
  axes = FALSE, xlab = "", ylab = "",
  main = "Correlation Matrix of Input Variables"
)
axis(1, at = 1:ncol(cor_X), labels = colnames(cor_X), las = 2)
axis(2, at = 1:ncol(cor_X), labels = colnames(cor_X), las = 2)
box()
```
```{r}
# Combine all input signals into one density plot
# ===============================

# Standardize inputs (mean = 0, sd = 1)
X_scaled <- scale(X)

# Colors for each variable
colors <- c("black", "red", "blue", "darkgreen", "purple", "orange")

# Plot density of first variable
plot(density(X_scaled[, 1], na.rm = TRUE),
     col = colors[1], lwd = 2,
     main = "Combined Density Plot of Input Signals (Standardized)",
     xlab = "Standardized Value",
     ylab = "Density")

# Add remaining density curves
for (i in 2:ncol(X_scaled)) {
  lines(density(X_scaled[, i], na.rm = TRUE),
        col = colors[i], lwd = 2)
}

# Legend
legend("topright",
       legend = colnames(X),
       col = colors,
       lwd = 2,
       cex = 0.8)
```

```{r}
#task1.3
# Scatter plots + LOESS smooth line
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

for (i in 1:6) {
  x <- X[, i]
  y <- as.numeric(Y)

  plot(x, y,
       main = paste("Scatter:", colnames(X)[i], "vs Y"),
       xlab = colnames(X)[i],
       ylab = "bg+1:00",
       pch = 16, cex = 0.6)

  ok <- complete.cases(x, y)
  lines(lowess(x[ok], y[ok]), col = "red", lwd = 2)
}

par(mfrow = c(1, 1))

# Install once (if not installed)
# install.packages("corrplot")

library(corrplot)

# Combine inputs (X) + output (Y) into one matrix/dataframe
df_all <- data.frame(X)
df_all$bg_plus_1 <- as.numeric(Y)   # name like your figure

# Correlation matrix
cor_mat <- cor(df_all, use = "complete.obs", method = "pearson")

# Plot: colored correlation matrix with values
corrplot(cor_mat,
         method = "color",
         type = "lower",
         addCoef.col = "black",
         number.cex = 0.8,
         tl.col = "black",
         tl.srt = 45,
         diag = TRUE,
         col = colorRampPalette(c("red","white","blue"))(200),
         cl.lim = c(-1, 1),
         title = "Correlation Matrix of Variables",
         mar = c(0, 0, 2, 0))
```

```{r}
# Task 2.1 – Least squares estimation via lm()
# make a clean response vector (length N)
y_vec <- as.numeric(Y)

# models as you already defined
df <- data.frame(
  y  = y_vec,
  x1 = X[, 1],
  x2 = X[, 2],
  x3 = X[, 3],
  x4 = X[, 4],
  x5 = X[, 5],
  x6 = X[, 6]
)

# Model 1: β1 x1^3 + β2 x2^2 + β3 x3^2 + β4 x4 + β5 x5 + β6 x6 + β0
m1 <- lm(y ~ I(x1^3) + I(x2^2) + I(x3^2) + x4 + x5 + x6, data = df)

# Model 2: β1 x1^2 + β2 x2^2 + β3 x3^3 + β4 x4 + β5 x5 + β6 x6 + β0
m2 <- lm(y ~ I(x1^2) + I(x2^2) + I(x3^3) + x4 + x5 + x6, data = df)


# Model 3: β1 x1 + β2 x2 + β3 x3 + β4 x4^2 + β5 x5 + β6 x6^2 + β0
m3 <- lm(y ~ x1 + x2 + x3 + I(x4^2) + x5 + I(x6^2), data = df)


# Model 4: β1 x1^2 + β2 x2^2 + β3 x3^2 + β4 x4^2 + β5 x5^2 + β6 x6^2 + β0
m4 <- lm(y ~ I(x1^2) + I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2), data = df)


# Model 5: β1 x1 + ... + β6 x6 + β7 x1x2 + β8 x3x4 + β9 x2x6 + β0
m5 <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + I(x1 * x2) + I(x3 * x4) + I(x2 * x6), data = df)

theta1 <- coef(m1)
theta2 <- coef(m2)
theta3 <- coef(m3)
theta4 <- coef(m4)
theta5 <- coef(m5)

X_m1 <- model.matrix(m1)
X_m2 <- model.matrix(m2)
X_m3 <- model.matrix(m3)
X_m4 <- model.matrix(m4)
X_m5 <- model.matrix(m5)

```
```{r}
# ===============================
# Pad coefficients to same length
# ===============================

max_len <- max(length(theta1), length(theta2), length(theta3),
               length(theta4), length(theta5))

pad <- function(x, n) c(x, rep(NA, n - length(x)))

theta_mat <- rbind(
  pad(theta1, max_len),
  pad(theta2, max_len),
  pad(theta3, max_len),
  pad(theta4, max_len),
  pad(theta5, max_len)
)

rownames(theta_mat) <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5")
colnames(theta_mat) <- paste0("θ", 0:(max_len - 1))

theta_mat
```

```{r}
#task 2.2 RSS calculation and Task 2.5
RSS_fun <- function(model) {
  res <- resid(model)          # residuals used to fit the model
  sum(res^2)
}

rss <- c(
  RSS_fun(m1),
  RSS_fun(m2),
  RSS_fun(m3),
  RSS_fun(m4),
  RSS_fun(m5)
)
names(rss) <- paste0("Model", 1:5)
rss

res1 <- resid(m1)
res2 <- resid(m2)
res3 <- resid(m3)
res4 <- resid(m4)
res5 <- resid(m5)

par(mfrow = c(2, 3))
qqnorm(res1, main = "QQ – Model 1"); qqline(res1, col = "red", lwd = 2)
qqnorm(res2, main = "QQ – Model 2"); qqline(res2, col = "red", lwd = 2)
qqnorm(res3, main = "QQ – Model 3"); qqline(res3, col = "red", lwd = 2)
qqnorm(res4, main = "QQ – Model 4"); qqline(res4, col = "red", lwd = 2)
qqnorm(res5, main = "QQ – Model 5"); qqline(res5, col = "red", lwd = 2)
par(mfrow = c(1, 1))


# Task 2.5 (extra): ACF of residuals
# ===============================

par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

acf(res1, main = "ACF – Residuals (Model 1)")
acf(res2, main = "ACF – Residuals (Model 2)")
acf(res3, main = "ACF – Residuals (Model 3)")
acf(res4, main = "ACF – Residuals (Model 4)")
acf(res5, main = "ACF – Residuals (Model 5)")

par(mfrow = c(1, 1))

# Task 2.5 (extra): Cook's distance
# ===============================

par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

plot(cooks.distance(m1), type = "h",
     main = "Cook's Distance – Model 1",
     ylab = "Cook's D", xlab = "Observation index")
abline(h = 4/N, col = "red", lty = 2)

plot(cooks.distance(m2), type = "h",
     main = "Cook's Distance – Model 2",
     ylab = "Cook's D", xlab = "Observation index")
abline(h = 4/N, col = "red", lty = 2)

plot(cooks.distance(m3), type = "h",
     main = "Cook's Distance – Model 3",
     ylab = "Cook's D", xlab = "Observation index")
abline(h = 4/N, col = "red", lty = 2)

plot(cooks.distance(m4), type = "h",
     main = "Cook's Distance – Model 4",
     ylab = "Cook's D", xlab = "Observation index")
abline(h = 4/N, col = "red", lty = 2)

plot(cooks.distance(m5), type = "h",
     main = "Cook's Distance – Model 5",
     ylab = "Cook's D", xlab = "Observation index")
abline(h = 4/N, col = "red", lty = 2)

par(mfrow = c(1, 1))
```

```{r}
#task2.3 log likelihood

#logLikFun <- function(rss, k) {
  #sigma2 <- rss / (N - 1)
  #-(N/2)*log(2*pi) - (N/2)*log(sigma2) - rss/(2*sigma2)
#}

#ogL <- sapply(rss, logLikFun)

logLikFun <- function(rss_val) {
  sigma2 <- rss_val / (N - 1)
  -(N/2) * log(2*pi) - (N/2) * log(sigma2) - rss_val / (2*sigma2)
}

logL <- sapply(rss, logLikFun)
```

```{r}
# Task 2.3: Log-likelihood table
# ===============================

logLikFun <- function(rss_val) {
  sigma2 <- rss_val / (N - 1)
  -(N/2) * log(2*pi) - (N/2) * log(sigma2) - rss_val / (2*sigma2)
}

# Compute log-likelihood for each model
logL <- sapply(rss, logLikFun)

# Create table
logLik_table <- data.frame(
  Model = names(logL),
  LogLikelihood = as.numeric(logL)
)

# Print table
print(logLik_table)
```

```{r}
#task 2.4 AIC and BIC
k <- c(ncol(X_m1), ncol(X_m2), ncol(X_m3),
       ncol(X_m4), ncol(X_m5))

AIC_vals <- 2 * k - 2 * logL
BIC_vals <- k * log(N) - 2 * logL

model_selection <- data.frame(
  Model = paste("Model", 1:5),
  parameters     = k,
  RSS   = rss,
  logL  = logL,
  AIC   = AIC_vals,
  BIC   = BIC_vals
)
model_selection
```

```{r}
#task 2.5 best model selection
  #best_model <- which.min(AIC)
  #best_model
best_model <- which.min(AIC_vals)
best_model
model_selection[best_model, ]
```

```{r}
#task 2.7 train-test + prediction interval
# ===============================
# Train-test split
# ===============================
split <- initial_split(data, prop = 0.7)
train <- training(split)
test  <- testing(split)

X_train <- as.matrix(train[,1:6])
Y_train <- as.matrix(train[,7])
X_test  <- as.matrix(test[,1:6])
Y_test  <- as.matrix(test[,7])

df_train <- data.frame(
  y  = Y_train,
  x1 = X_train[, 1],
  x2 = X_train[, 2],
  x3 = X_train[, 3],
  x4 = X_train[, 4],
  x5 = X_train[, 5],
  x6 = X_train[, 6]
)

# Example: use Model 3
m5_train <- lm(
  y ~ x1 + x2 + x3 + x4 + x5 + x6 + I(x1 * x2) + I(x3 * x4) + I(x2 * x6), data = df_train)

summary(m5_train)

theta_hat <- coef(m5_train)
Xm_train  <- model.matrix(m5_train)

df_test <- data.frame(
  x1 = X_test[, 1],
  x2 = X_test[, 2],
  x3 = X_test[, 3],
  x4 = X_test[, 4],
  x5 = X_test[, 5],
  x6 = X_test[, 6]
)
Xm_test <- model.matrix(
  ~ x1 + x2 + x3 + x4 + x5 + x6 + I(x1 * x2) + I(x3 * x4) + I(x2 * x6), data = df_test
)

Y_pred <- as.numeric(Xm_test %*% theta_hat)
error  <- Y_test - Y_pred
ci     <- 1.96 * sd(error)
ci
```
```{r}
# Task 2.7: Train–test + Prediction Interval (Model 5) again
# ===============================

library(rsample)

# -------------------------------
# Train-test split
# -------------------------------
split <- initial_split(data, prop = 0.7)
train <- training(split)
test  <- testing(split)

X_train <- as.matrix(train[, 1:6])
Y_train <- train[, 7]

# Distribution plot of Training Data (Y_train)
# ===============================

y_train_clean <- as.numeric(Y_train)
y_train_clean <- y_train_clean[!is.na(y_train_clean)]

dens <- density(y_train_clean)

plot(dens,
     main = "Distribution of Training Data",
     xlab = "Y (bg+1:00)",
     ylab = "Density",
     lwd = 2)

# Add mean line
abline(v = mean(y_train_clean), col = "red", lwd = 2, lty = 2)

# Add median line (optional, but useful)
abline(v = median(y_train_clean), col = "blue", lwd = 2, lty = 3)

# Add small note like the sample
mtext(paste("N =", length(y_train_clean),
            " Bandwidth =", round(dens$bw, 4)))

X_test <- as.matrix(test[, 1:6])
Y_test <- test[, 7]

# -------------------------------
# Training dataframe
# -------------------------------
df_train <- data.frame(
  y  = Y_train,
  x1 = X_train[, 1],
  x2 = X_train[, 2],
  x3 = X_train[, 3],
  x4 = X_train[, 4],
  x5 = X_train[, 5],
  x6 = X_train[, 6]
)

# -------------------------------
# Model 5 (with interactions)
# -------------------------------
m5_train <- lm(
  y ~ x1 + x2 + x3 + x4 + x5 + x6 +
       I(x1 * x2) + I(x3 * x4) + I(x2 * x6),
  data = df_train
)

summary(m5_train)
# Distribution of Training Residuals (Model 5)
# ===============================

res_train <- resid(m5_train)
res_train <- res_train[!is.na(res_train)]

dens_res <- density(res_train)

plot(dens_res,
     main = "Distribution of Training Residuals (Model 5)",
     xlab = "Residuals",
     ylab = "Density",
     lwd = 2)

# Mean (should be close to zero)
abline(v = mean(res_train), col = "red", lwd = 2, lty = 2)

# Median
abline(v = median(res_train), col = "blue", lwd = 2, lty = 3)

mtext(paste("N =", length(res_train),
            " Bandwidth =", round(dens_res$bw, 4)))

# -------------------------------
# Test dataframe
# -------------------------------
df_test <- data.frame(
  x1 = X_test[, 1],
  x2 = X_test[, 2],
  x3 = X_test[, 3],
  x4 = X_test[, 4],
  x5 = X_test[, 5],
  x6 = X_test[, 6]
)

# -------------------------------
# Prediction with 95% prediction interval
# -------------------------------
pred_PI <- predict(
  m5_train,
  newdata = df_test,
  interval = "prediction",
  level = 0.95
)

# Convert to data frame for clarity
pred_PI <- as.data.frame(pred_PI)

# -------------------------------
# Combine with true Y for evaluation
# -------------------------------
results <- data.frame(
  Y_true = Y_test,
  Y_pred = pred_PI$fit,
  PI_lower = pred_PI$lwr,
  PI_upper = pred_PI$upr
)
# Plot: Test data vs Prediction with 95% Prediction Intervals (error bars)
# ===============================

# X-axis index for test samples (keeps correct order in the test set)
idx <- 1:nrow(results)

plot(idx, results$Y_true,
     pch = 16, cex = 0.6,
     main = "Test Data vs Prediction (Model 5) with 95% Prediction Interval",
     xlab = "Test sample index",
     ylab = "Y (bg+1:00)")

# Add prediction line
lines(idx, results$Y_pred, lwd = 2)

# Add error bars (prediction interval)
arrows(idx, results$PI_lower,
       idx, results$PI_upper,
       angle = 90, code = 3, length = 0.03)

legend("topleft",
       legend = c("Test Y", "Predicted Y", "95% PI"),
       pch = c(16, NA, NA),
       lty = c(NA, 1, NA),
       bty = "n")

# Cleaner zoomed plot (first 300 test points)
m <- min(300, nrow(results))
idx2 <- 1:m

plot(idx2, results$Y_true[idx2],
     pch = 16, cex = 0.6,
     main = "Zoom: Test vs Prediction (first 300 samples) with 95% PI",
     xlab = "Test sample index",
     ylab = "Y (bg+1:00)")

lines(idx2, results$Y_pred[idx2], lwd = 2)

arrows(idx2, results$PI_lower[idx2],
       idx2, results$PI_upper[idx2],
       angle = 90, code = 3, length = 0.03)

# Test set error metrics
# ===============================

errors <- results$Y_true - results$Y_pred

RMSE <- sqrt(mean(errors^2))
MAE  <- mean(abs(errors))

legend("bottomright",
       legend = c(
         paste("RMSE =", round(RMSE, 6)),
         paste("MAE =", round(MAE, 6))
       ),
       bty = "n")

cat("Test set performance metrics:\n")
cat("RMSE =", round(RMSE, 6), "\n")
cat("MAE  =", round(MAE, 6), "\n")


head(results)

```

```{r}
#task 3 Approximate baysian computation (ABC)
# ===============================
# ABC – Rejection Sampling
# ===============================
theta_star <- theta_hat
param1 <- 1
param2 <- 2

samples <- 1000
accepted <- matrix(NA, nrow = samples, ncol = 2)
eps <- rss[best_model] * 1.5

count <- 0
for (i in 1:samples) {
  theta_new <- theta_star
  theta_new[param1] <- runif(1, theta_star[param1]-0.1, theta_star[param1]+0.1)
  theta_new[param2] <- runif(1, theta_star[param2]-0.1, theta_star[param2]+0.1)
  
  Y_sim <- Xm_train %*% theta_new
  rss_new <- sum((Y_train - Y_sim)^2)
  
  if (rss_new < eps) {
    count <- count + 1
    accepted[count,] <- theta_new[c(param1,param2)]
  }
}

accepted <- accepted[1:count, , drop = FALSE]
plot(accepted, main = "ABC Posterior Distribution")
```

```{r}
# Task 3: Rejection ABC (2 parameters)
# ===============================

set.seed(123)

# theta_hat: coefficients of selected model (e.g., theta5)
theta_star <- theta_hat   # make sure this matches selected model coefficients
k <- length(theta_star)

# Xm_train: design matrix for selected model on TRAIN data
# Example: Xm_train <- model.matrix(m5_train)
# Ensure it matches theta_star order/dimension:
stopifnot(ncol(Xm_train) == length(theta_star))

# -------------------------------
# 1) Select 2 parameters with largest absolute value
#    (exclude intercept if you want; many assignments keep it included)
# -------------------------------
abs_theta <- abs(theta_star)
top2 <- order(abs_theta, decreasing = TRUE)[1:2]
param1 <- top2[1]
param2 <- top2[2]

cat("Selected parameters (by |theta_hat|):", param1, param2, "\n")
cat("theta_hat values:", theta_star[param1], theta_star[param2], "\n")

# -------------------------------
# 2) Uniform priors around theta_hat for the 2 params
#    Use relative width (e.g., +/- 20% of |theta_hat|)
# -------------------------------
rel_width <- 0.20
w1 <- max(0.01, rel_width * abs(theta_star[param1]))
w2 <- max(0.01, rel_width * abs(theta_star[param2]))

prior_bounds <- list(
  p1 = c(theta_star[param1] - w1, theta_star[param1] + w1),
  p2 = c(theta_star[param2] - w2, theta_star[param2] + w2)
)

# -------------------------------
# 3) Choose epsilon using prior predictive distances (recommended)
# -------------------------------
S0 <- 3000
rss0 <- numeric(S0)

for (i in 1:S0) {
  th <- theta_star
  th[param1] <- runif(1, prior_bounds$p1[1], prior_bounds$p1[2])
  th[param2] <- runif(1, prior_bounds$p2[1], prior_bounds$p2[2])

  y_sim <- as.numeric(Xm_train %*% th)
  rss0[i] <- sum((Y_train - y_sim)^2)
}

# keep the best ~2% as acceptance threshold (adjust 0.01–0.05)
eps <- quantile(rss0, probs = 0.02)
cat("Epsilon (2% quantile):", eps, "\n")

# -------------------------------
# 4) Rejection ABC sampling
# -------------------------------
samples <- 10000
accepted <- matrix(NA, nrow = samples, ncol = 2)
count <- 0

for (i in 1:samples) {
  th <- theta_star
  th[param1] <- runif(1, prior_bounds$p1[1], prior_bounds$p1[2])
  th[param2] <- runif(1, prior_bounds$p2[1], prior_bounds$p2[2])

  y_sim <- as.numeric(Xm_train %*% th)
  rss_new <- sum((Y_train - y_sim)^2)

  if (rss_new <= eps) {
    count <- count + 1
    accepted[count, ] <- th[c(param1, param2)]
  }
}

accepted <- accepted[1:count, , drop = FALSE]
cat("Accepted samples:", count, " (rate =", round(count/samples, 4), ")\n")

# -------------------------------
# 5) Plots: Joint + marginals (with prior overlay)
# -------------------------------

# Joint posterior
plot(accepted[,1], accepted[,2],
     pch = 16, cex = 0.5,
     xlab = paste0("theta[", param1, "]"),
     ylab = paste0("theta[", param2, "]"),
     main = "Joint Posterior (Rejection ABC)")

# Marginals
par(mfrow = c(1, 2))

# param1 marginal
hist(accepted[,1], breaks = 30, probability = TRUE,
     main = paste0("Posterior of theta[", param1, "]"),
     xlab = paste0("theta[", param1, "]"))
lines(density(accepted[,1]), lwd = 2)

# overlay prior (uniform density)
curve(dunif(x, prior_bounds$p1[1], prior_bounds$p1[2]),
      add = TRUE, lty = 2)

# param2 marginal
hist(accepted[,2], breaks = 30, probability = TRUE,
     main = paste0("Posterior of theta[", param2, "]"),
     xlab = paste0("theta[", param2, "]"))
lines(density(accepted[,2]), lwd = 2)
curve(dunif(x, prior_bounds$p2[1], prior_bounds$p2[2]),
      add = TRUE, lty = 2)

par(mfrow = c(1, 1))

```

```{r}
# Acceptance rate
# ===============================
acceptance_rate <- count / samples
cat("Acceptance rate:", round(acceptance_rate * 100, 2), "%\n")

#trace-style plot
par(mfrow = c(2, 1))

plot(
  accepted[,1], type = "l",
  main = expression("Trace plot of " * beta[1]),
  xlab = "Accepted sample index",
  ylab = expression(beta[1])
)

plot(
  accepted[,2], type = "l",
  main = expression("Trace plot of " * beta[2]),
  xlab = "Accepted sample index",
  ylab = expression(beta[2])
)

par(mfrow = c(1, 1))

#compare ABC posterior vs normal(MLE-based)
# Normal approximation posterior
# ===============================
sigma2_hat <- summary(m5_train)$sigma^2
cov_theta  <- sigma2_hat * solve(t(Xm_train) %*% Xm_train)

mu1 <- theta_hat[param1]
sd1 <- sqrt(cov_theta[param1, param1])

mu2 <- theta_hat[param2]
sd2 <- sqrt(cov_theta[param2, param2])

#overlay marginal posteriors(ABC vs Normal)
par(mfrow = c(1, 2))

# β1
hist(
  accepted[,1], breaks = 30, probability = TRUE,
  main = expression(beta[1] * ": ABC vs Normal"),
  xlab = expression(beta[1])
)
curve(dnorm(x, mu1, sd1), col = "red", lwd = 2, add = TRUE)

# β2
hist(
  accepted[,2], breaks = 30, probability = TRUE,
  main = expression(beta[2] * ": ABC vs Normal"),
  xlab = expression(beta[2])
)
curve(dnorm(x, mu2, sd2), col = "red", lwd = 2, add = TRUE)

par(mfrow = c(1, 1))
```